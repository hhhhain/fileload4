__global__ void CalDetection(const float *input, float *output, int noElements,
    const int netwidth, const int netheight, int maxoutobject, int yoloWidth,
    int yoloHeight, const float anchors[kNumAnchor * 2], int classes, int outputElem, bool is_segmentation) {
  int idx = threadIdx.x + blockDim.x * blockIdx.x;
  int tid = threadIdx.x + blockDim.x * blockIdx.x;

  if (idx >= noElements) return; 

  int total_grid = yoloWidth * yoloHeight;
  // printf("total_grid is %d\n", total_grid);
  int bnIdx = idx / total_grid; // batch的索引，相当于第几张图，10张图的第几张。
  idx = idx - total_grid * bnIdx;// 是偏移。相对于当前batch图像的起始位置的网格索引。
  int info_len_i = 5 + classes; //每个anchor的元素数量，这个好理解，比如5+80.5是由boss，conf组成。
  if (is_segmentation) info_len_i += 32; //分割。
  // kNumAnchor如前文猜测应该是每个head的anchor数，乘以格子数，乘以每个格子的信息量，再乘以第几张图，然后对input指针进行偏移，指向某一张图的起始位置。
  const float* curInput = input + bnIdx * (info_len_i * total_grid * kNumAnchor); 
  // kNumAnchor如前文猜测应该是每个head的anchor数，每个cell同样，比如是3. 遍历每个anchor， 
  for (int k = 0; k < kNumAnchor; ++k) {
    // 这条语句是在对某一张图的所有cell求置信度，0或者1. 为什么是所有cell？因为每一个idx都绑定了线程，都是并行执行的。[anchor1][anchor2][anchor3]的顺序在内存中存放。
    float box_prob = Logist(curInput[idx + k * info_len_i * total_grid + 4 * total_grid]);
    // 所有cell都是并行的，所以如果判断出当前的cell的置信度太低的话，就不管这个cell了。执行下一次for，也就是下一个anchor，同理。
    if (box_prob < kIgnoreThresh) continue;
    int class_id = 0;
    float max_cls_prob = 0.0;
    // 遍历80个类，找到最大的可能性，就是class。同样是每个cell并行的。
    for (int i = 5; i < 5 + classes; ++i) {
      float p = Logist(curInput[idx + k * info_len_i * total_grid + i * total_grid]);
      if (p > max_cls_prob) {
        max_cls_prob = p;
        class_id = i - 5;
      }
    }
    // 第几张乘以单张的大小，res_count指当前这一张的输出起点。第一位res_count[0]记录了已写入的数量。
    // atomicAdd是原子加法，防止多线程写入同一个地址造成覆盖等。res_count总数+1，比如已经写了1223个detection结果。detection是上面说的结构体：坐标、置信度、类别。
    // 这里有个问题，这里的超过框的总数就不写了，是怎么一个并行情况？都在抢着写？怎么一个层次去抢的？初步感觉是所有的cell都在判断最小的anchor，从小到大了写。
    float *res_count = output + bnIdx * outputElem;
    int count = (int)atomicAdd(res_count, 1);
    if (count >= maxoutobject) return;
    // 每个cell计算自己应该要写的位置，转成detection指针，以后要写的时候就可以用det->结构。比如下面。
    char *data = (char*)res_count + sizeof(float) + count * sizeof(Detection);
    Detection *det = (Detection*)(data);
    if (tid < 3) {
        printf("Thread %d: res_count address: %p, value: %f, atomic count: %d\n", tid, res_count, *res_count, count);
    }

    // 如果 tid >=3 不想执行后续逻辑，就直接 return
    if (tid >= 3) return;

    // 定位行和列。   
    // printf("yoloWidth is %d\n, yoloHeight is %d\n", yoloWidth, yoloHeight);  
    int row = idx / yoloWidth;
    int col = idx % yoloWidth;
    
    // 定位中心坐标。其实就是col/row - 0.5 + 2 * sigmoid。而sigmoid在01之间，所以col/row + (-0.5~1.5), 所以是相对于格子左上角，可以左偏0.5格子到右偏1.5个格子。然后* netwidth / yoloWidth是转换回原图坐标系。
    // 这里计算的是框中心x和y。
    det->bbox[0] = (col - 0.5f + 2.0f * Logist(curInput[idx + k * info_len_i * total_grid + 0 * total_grid])) * netwidth / yoloWidth;
    det->bbox[1] = (row - 0.5f + 2.0f * Logist(curInput[idx + k * info_len_i * total_grid + 1 * total_grid])) * netheight / yoloHeight;

    // 2.0f * sigmoid映射到0到2之间。
    det->bbox[2] = 2.0f * Logist(curInput[idx + k * info_len_i * total_grid + 2 * total_grid]);
    // 最大是4倍anchor，最小当然是就是0了，靠sigmoid调控。宽高同理。
    // 比较重要的是为什么中心点* netwidth / yoloWidth映射回去了，而宽高不需要映射。因为anchor box本身就是在输入特征图大小考虑的，比如640，640. 多少倍的anchor长宽，得到的框数据就是640*640的原图长宽。
    // 那为什么中心点需要映射回去呢？因为前向传播推理得到的中心点是基于输出特征图的cell的，不是基于640 640这个输入特征图的。所以要把中心点从输出特征图的cell映射回640 640这个输入特征图。
    det->bbox[2] = det->bbox[2] * det->bbox[2] * anchors[2 * k];
    det->bbox[3] = 2.0f * Logist(curInput[idx + k * info_len_i * total_grid + 3 * total_grid]);
    det->bbox[3] = det->bbox[3] * det->bbox[3] * anchors[2 * k + 1];

    // 有物体的置信度乘以这个物体是啥的分类得分，就是联合置信度conf。
    det->conf = box_prob * max_cls_prob;
    det->class_id = class_id;

    for (int i = 0; is_segmentation && i < 32; i++) {
      det->mask[i] = curInput[idx + k * info_len_i * total_grid + (i + 5 + classes) * total_grid];
    }
  }
}
    
fp16：


fp16+sp10：
7.4365216255187985
3.024086427688599
2.5158794562021893
2.5974136114120485
2.497176303863525
2.4584336121877035
2.431328936985561
2.4528656005859375
2.4175214131673175
2.3988800048828125
2.381844659285112
2.409696515401204
2.394891797579252
2.3814827237810405
2.3723579915364583
2.3994184255599977
2.410098109525793
2.395954852634006
2.4139292867560136
2.4967110443115237
2.4253880455380394
2.4222260041670367
2.443262083634086
2.462102270126343
2.4844857482910156
2.4759259003859304
2.525197036178024
2.54049630846296
2.5620315551757815
2.576527786254883
2.5980724457771545



sp20：
6.639420747756958
3.093023991584778
2.334381850560506
2.334157609939575
2.2495520019531248
2.2318005243937176
2.1797051565987724
2.2056679725646973
2.1772757212320966
2.1764441680908204
2.1401937658136543
2.1618410905202228
2.1384447978093073
2.137576689038958
2.1200589116414386
2.124501180648804
2.1424017289105586
2.1366714477539066
2.1524463452790914
2.153523368835449
2.156316175914946
2.1437819740988995
2.2005615234375
2.157050641377767
2.178324996948242
2.173839422372671
2.2050793400517215
2.197294534955706
2.207647441995555
2.242806930541992
2.2474736367502524

sp30：
5.8988128185272215
2.5530511856079103
2.0061962604522705
1.9550855994224547
1.946367988586426
1.9278704007466636
1.8917814799717494
1.9067935943603516
1.8840594185723196
1.8890726470947263
1.8658047762784091
1.8716642697652182
1.8576046723585864
1.8539289202008928
1.8383568954467775
1.8571599841117858
1.854118403266458
1.8474151187472871
1.8518848419189453
1.8531712150573731
1.873695845831008
1.8492142590609466
1.855752480548361
1.8647283871968587
1.8735751647949217
1.8713666475736177
1.8850648668077257
1.8879790987287248
1.9183079818199422
1.892912737528483
1.9163015057963708

sp40：
5.020579242706299
2.637705612182617
1.9647999922434487
1.8078904151916504
1.7670355606079102
1.7425066471099855
1.7236461911882672
1.7333376049995421
1.7163562774658203
1.71962589263916
1.7070245742797852
1.7069613456726074
1.690984139075646
1.6914754050118581
1.6801668294270833
1.6900796055793763
1.6940231547636144
1.6799514664544
1.6856394014860454
1.6829958343505858
1.6875437600272043
1.6845130226828835
1.6812734686810038
1.6858145395914714
1.6842550811767578
1.6967258013211763
1.6939508226182725
1.7020109585353307
1.704142563918541
1.7170777638753256
1.7208387067241053

sp50：
3.6004608154296873
2.1599823951721193
1.6190346797307331
1.2161391973495483
1.0823750495910645
1.0633728186289468
1.043105833871024
1.0657739877700805
1.0528014183044432
1.0404204750061035
1.0334871205416591
1.0373879988988242
1.0278779029846192
1.0194256033216205
1.0154011726379395
1.0328366041183472
1.0280141269459444
1.0218709415859646
1.0233751497770611
1.0174828815460206
1.0122698738461449
1.0086158579046076
1.0129270387732465
1.0094079971313477
1.005639678955078
1.0027337881234977
1.0049349961457428
1.000990056991577
0.9989171850270239
0.9959839057922364
1.004957531344506

int8，HTOD+INFER
7.12740797996521
4.475588846206665
3.9075296084086104
3.8601903915405273
3.769957733154297
3.723099199930827
3.7332324436732702
3.716474008560181
3.7810005611843533
3.7009954833984375
3.6503778631036927
3.686542924245199
3.6523778181809643
3.643160901750837
3.6893175252278647
3.6587657928466797
3.7229474235983453
3.7268108367919925
3.6544818677400284
3.649037742614746
3.659470040457589
3.6379772533069956
3.6592956211255943
3.614420509338379
3.626671203613281
3.6049352205716647
3.660767760100188
3.636604418073382
3.651482522898707
3.6568747965494794
3.6443435422835813

fp16，HTOD+INFER
7.442911958694458
5.064220809936524
3.828957939147949
3.8338296175003053
3.7572467041015627
3.718008009592692
3.6724068777901784
3.7785563707351684
3.7884383731418185
3.6552691268920894
3.6600596341219815
3.6804931322733565
3.624006916926457
3.6154267447335378
3.6732669576009114
3.6101085901260377
3.609425511079676
3.60220472547743
3.6127306084883837
3.6972543716430666
3.5967599777948287
3.6358936656605114
3.6250891312308933
3.629015032450358
3.608347503662109
3.6017909123347356
3.6682775426793985
3.6114528928484235
3.636135522250471
3.672622858683268
3.652595618463332
